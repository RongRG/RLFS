\include{RLFS.preambel}

\title{Deep RL and Tree-search for Feature Selection}
\author{Wendelin B\"ohmer (TU Delft), Rong Guo (TU Berlin)}

% ==============================================================================

\begin{document}
\maketitle

\listoftodos

% ==============================================================================
\section{Domain Background}

\begin{center}
\begin{longtable}{| m{4cm} | m{5cm} | m{5cm} | }
\hline
\textbf{Paper} & \textbf{Summary} & \textbf{Drawbacks w.r.t ours}\\
\hline
\hline
EDDI \citep{ma_eddi_2019} & (Partial) variational autoencoders + an information theoretic acquisition function that seeks to maximize the expected information gain over a set of unobserved variables & \href{https://slideslive.com/38931299/efficient-missingvalue-acquisition-with-variational-autoencoders?ref=account-folder-55866-folders}{Efficient Missing-value Acquisition with Variational Autoencoders} \\
\hline
ODIN \citep{zannone_odin_2019} & Use Partial VAE to learn the generative distribution of the data & Model-based RL; The state-model handles missing data; Same reward function as ours\\
\hline
Icebreaker \citep{gong_icebreaker_2019} & amortized inference is used for local latent variables and sampling methods used for model parameters & deal with the ice start problem in active learning \\
\hline
\hline
Learning to Look Around \citep{jayaraman_learning_2018} & active observation completion (unseen environments, unknown tasks) &  scene understanding task, independent of a recognition task\\
\hline
Glance and Focus \citep{wang_glance_2020} & reduce the spatial redundancy in image classification tasks, CNN + RNN & down-sampled version of the original image or a cropped patch\\
\hline
Recurrent Models of Visual Attention \citep{mnih_recurrent_2014} & end-to-end optimization of the sequential decision process,  a single recurrent neural network & uses the glimpse sensor to extract retina representation\\
\hline
\hline
AlphaZero \citep{silver_general_2018} & tree search and search-based policy iteration & 
	\begin{align}
	   \Set L = (z-v)^2 - \pi^T\log(P) + c\|\theta\|^2
	\end{align}
	The $v, \pi$ guides the MCTS, while MCTS can be viewed as a way of building $v, \pi$ (policy iteration). \\
\hline
\hline
Feature Selection as a One-Player Game \citep{gaudel_feature_2010} & other non-deep methods & MCTS \\
\hline
\end{longtable}
\end{center}


% ==============================================================================
\section{Problem/Solution Statement}

What is the main challenge for AFA? What are the main drawbacks of the existing approaches? Why RL is necessary?\\

Significance of the problem: real-world domains where data is minimal and collection is expensive.\\

Technical novelty? not all original, but are rather an interesting use-case/application of existing methods and are a useful contribution.\\ 


Feature selection as a reinforcement learning problem \citep{gaudel_feature_2010}, which simultaneously maximize the prediction accuracy and the feature acquisition returns.

Given a feature set $\Set A$, the state space of the MDP is the powerset of $\Set A$. We define a tree $\Set T$ where each node $\ve x$ correspond to a state, $\ve x \subset \Set A$. The action is to select a child node $\ve x_c$ of the parent node $\ve x_p$. The node sets are defined as $\{\ve x_c \subset \Set A, \exists ~ a \in \Set A \backslash \ve x, \ve x_c = \ve x \cup \{a\}\}$ and $\{\ve x_p \subset \Set A, \exists ~ a \in \ve x, \ve x_p = \ve x \backslash \{a\}\}$. The number of nodes in the tree is $2^{|\Set A|}$.\\

Goal: FS $\rightarrow$ generalization error. The features are used in the classification/regression model. Feature selection and model construction are combined in one end-to-end process.\\


Bellman optimality

\begin{enumerate}
\item
Quantifiable: expressed in math/logic terms
\item
Measurable: metric
\item
Repliable
\end{enumerate}

% ==============================================================================
\section{Dataset \& Input \& Metric}
\begin{enumerate}
\item
	\todo[inline]{Environment/Experiments.}
	\begin{itemize}
	\item
	CUBE-$\delta$
	\item
	MNIST
	\item
	IMAGENET
	\item
	Diagnostic tests in medical settings: \href{https://mimic.mit.edu}{MIMIC}, \href{https://archive.ics.uci.edu/ml/index.php}{UCI}
	\end{itemize}

\item
	\todo[inline]{Benchmark model \& results, which contextualises existing methods/knowledge in the domain.}
	\begin{itemize}
	\item
	SOTA: How are the existing work related to our methods? 
	\item
	Additional experiments contrasting with non-deep learning methods
	\end{itemize}
\item
	\todo[inline]{Metric: objectively compare the solution model with the benchmark model.}
	\begin{itemize}
	\item
	Accuracy vs. \# of acquired features
	\item
	ROC AUC vs. \# of acquired features\\
	AUIC: area under the information curve\\
	Negative log likelihood (for Imputation task)
	\item
	Time complexity
	\end{itemize}
\end{enumerate}

% ==============================================================================
\section{Theoretical workflow}
\subsection{End-to-end formalization}
We are given a training set \hlfix{$\{\ve x^i\}_{i=1}^n \subset \R^{|\Set A|}$}{sets: index and value}
of feature-vectors (e.g.~MNIST images).
Our task is to classify the corresponding labels $\{y^i\}_{i=1}^n$
(e.g.~shown numbers $0 \ldots 9$) by maximizing 
the probability $p_\theta(y^i|\ve x^i)$ parameterized by $\theta$.
However, the classification shall be performed with very few features,
which we can select in an iterative process.

Starting with a fully hidden vector $\ve x^i_0$, 
a stochastic policy $\pi_\phi(a|\ve x^i_t)$, parameterized by $\phi$,
will select which feature $a \in \Set A$ will be revealed next.
This implies a transition model $P(\ve x^i_{t+1}|\ve x^i_{t}, a)$, 
which reveals feature $a$ of masked vector $\ve x^i_t$.
The cross-entropy classification loss after $t$ time steps is
\begin{align}
	\Set L_t := -\smallfrac{1}{n} \smallsum{i=1}{n}\,  
	\E_\pi\Big[ \ln p_\theta(y^i|\ve x^i_t) \Big]
	\quad \text{where} \quad
	\E_\pi\Big[f(\ve x^i_t)\Big] := \E\Big[ f(\ve x^i_t) \,\Big|
		\stack{
		a_k \sim \pi_\phi(\ve x_k^i)
		\brs[-5mm]
		\ve x^i_{k+1} \sim P(\ve x^i_k, a_k)
		}\!, 0 \leq k < t
	\Big] \,.
\end{align}
We can optimizes all classifications after up to $T$ reveals 
in one end-to-end loss functions:
\begin{align}
	\Set L \;:=\; \sum_{t=\new{1}}^T \Set L_t \;=\; 
	-\smallfrac{1}{n} \sum_{i=1}^{n}\,  
	\underbrace{\E_\pi\Big[ \smallsum{t=\new{1}}{T} 
		\overbrace{\ln p_\theta(y^i| \ve x^i_t)}^{r^i_\new{t-1}} 
	\Big]}_{J^\pi} \,.
\end{align}
Note that this is also the objective $J^\pi$ of reinforcement learning
with rewards
\hlfix{ $r^i_t := \ln p_\theta(y^i|\ve x^i_{t+1})$}{ $r^i_t := \ln p_\theta(y^i|\ve x^i_{t+1}) - T \cdot c_t$} ~
We can therefore optimize the policy $\pi_\phi$
with the REINFORCE algorithm \citep{Williams92}:
\begin{align}
	\subgrad{\phi} \Set L \;=\; 
	-\smallfrac{1}{n} \sum_{i=1}^{n} \sum_{t=\new 1}^{T}\,  
	\E_\pi\Big[ 
		\ln p_\theta(y^i| \ve x^i_t)  \,
		\subgrad{\phi} \ln \pi_\phi(a_\new{t-1}| \ve x^i_\new{t-1}) \Big] \,.
\end{align}
To learn both models simultaneously end-2-end, we can therefore use the 
combined loss\hlfix{\footnote{Where $\bot$ stops the gradient flow, e.g.~using \texttt{detach()} in \texttt{pytorch}.}}{?}:
\begin{align}
	\Set L^\text{e2e} \quad:=\quad
	- \E_\pi\Big[
	\smallfrac{1}{n} \sum_{i=1}^{n} \sum_{t=\new 1}^{T}\,  \Big(
	 \ln p_\theta(y^i| \ve x^i_t) 
	+ \lambda \bot\big(\ln p_\theta(y^i| \ve x^i_t)\big)  \,
		\ln \pi_\phi(a_\new{t-1}| \ve x^i_\new{t-1}) \Big) \Big]\,.	
\end{align}
Note that the choice of $\lambda$ is irrelevant, 
unless the two both models $p_\theta$ and $\pi_\phi$ share parameters.
One could also use actor-critic algorithms \citep{Sutton00}.
However, the downside is that all samples have to be 
{\em on-policy}, that is, sampled by the current policy $\pi_\theta$.
This is slow and can lead to catastrophic forgetting
\citep[which can be counteracted by modern 
algorithms,][but this is another story]{Schulman15, Schulman17}.


% ==============================================================================
\subsection{Using tree search}
Starting with AlphaGoZero \citep{silver_mastering_2017}, 
Deepmind's work on AlphaGo \citep{silver_mastering_2016}
has moved away from traditional reinforcement learning
and uses in their latest iterations \citep{silver_general_2018, schrittwieser_mastering_2020}
a classification loss instead:
\begin{align}
	\Set L' \quad:=\quad
	-\smallfrac{1}{n} \sum_{i=1}^{n} \sum_{t=0}^{\new{T-1}}\,  
	\E_{\Set T}\Big[ \ln \pi_\phi(a_t| \ve x^i_t) \Big] \,,
\end{align}
where $\Set T(a|\ve x^i_t)$ denotes the tree-search operator
starting at, or continuing from, $\ve x^i_t$. 
On the surface tree-search often yields an already good policy,
which is then distilled into the policy $\pi_\phi$.
However, this way $\pi_\phi$ can never become better than $\Set T$.
If not the entire search-tree can be computed efficiently,
and the performance of $\Set T$ is therefore often suboptimal.
\citet{silver_mastering_2017} argue that this changes fundamentally 
when we can use $\pi_\phi$ in $\Set T$ (now denoted $\Set T_\pi$).
If we can guarantee that $\Set T_\pi$ is a {\em policy improvement 
operator}, i.e.~$\Set L(\Set T_\pi) \geq \Set L(\pi_\phi)$,
the learned policy will keep improving.
This can be guaranteed by making sure that the 
action sequences that would be likely under $\pi_\phi$
are included in the tree search first,
e.g.~by implementing a preferential expansion heuristic
(policy orders actions during node expansion)
or by drawing the first $m$ actions in any expanded node
from the policy $\pi_\phi$ (e.g. by making $\pi_\phi$ a softmax). 

Keep in mind that this does not allow to train the classifier
with the same loss function, but one should still consider
adjusting the classifier to the distribution induced by
the learned policy $\pi_\phi$,
e.g., by minimizing $\Set L^\text{ts} := \Set L + \lambda \Set L'$.

% ==============================================================================
\subsection{On-policy learning for tree-search}
Using tree-search $\Set T(a^i_t|\ve x^i_t)$ 
on individual samples $\ve x^i$
induces another problem that 
Cheng was pointing out in our discussion: 
different samples $i$ can have different optimal actions $a$ at time $t$.
Training with actions found by a tree-search 
will therefore yield the distribution 
\begin{align}
	\xi(a_t|\ve x_t) 
	\quad:=\quad \smallfrac{1}{n}\sum_{i=1}^n \,
	\mathbb{I}[\Set T(a_t| \ve x^i_t) = 1] \,,
\end{align}
where $\mathbb{I}$ denotes the indicator operator, 
which is 1 if the argument is true and 0 otherwise. 
This induces two problems:
\begin{enumerate}
	\item In difference to tree search, 
		which always knows the current sample $\ve x^i$
		and therefore which feature should be revealed, 
		the classification loss $\Set L'$
		uses the expectation $\E_{\Set T}$ to imitate $\xi$.
		This means the learned policy $\pi$ 
		will have to make some choices randomly when 
		faced with unknown $\ve x_t$.
		These decisions will lead to states $\ve x^i_t$
		that the decision tree would not have visited
		and which is the policy has therefore not been trained for. 
	\item The distribution $\xi(a_t|\ve x_t)$ may not be the optimal
		for an unknown partially revealed $\ve x_t$,
		as another action, that is suboptimal for each image,
		may reveal the most information to distinguish between 
		samples and would therefore be optimal for an unknown $\ve x_t$.
\end{enumerate}

\subsubsection{On-policy learning}
First, note that both problems appear due to tree-search.
Minimizing the end-to-end loss $\Set L^\text{e2e}$
should learn to select actions that optimize the long term
accuracy for all samples and do this {\em on-policy}
by training on the rollouts of the policy itself.
We can introduce the latter to tree-search
by sampling $\ve x_t^i$ with the learned policy, 
but selecting the targets with $\Set T$:
\begin{align}
	\Set L'' \quad:=\quad
	-\smallfrac{1}{n} \sum_{i=1}^{n} \sum_{t=0}^{T-1}\,  
	\E_{\pi}\Big[ \ln \pi_\phi(\bar a_t| \ve x^i_t) 
		\,\Big|\, \bar a_t \sim \Set T(\cdot|\ve x_t^i) \Big] \,.
\end{align}
Note that this objective will still imitate $\xi$,
but on the entire distribution of samples reachable by $\pi_\phi$.
For tree-search algorithms that only output 
an optimal sequence of actions, 
this would require to rerun the search whenever 
the policy decides to alter that sequence.
The same is advisable in regular intervals
if the tree search only returns approximately optimal sequences
(which have seen less data deeper in the tree).

\subsubsection{Fixed tree search}
The second problem above is harder to solve with tree-search.
To demonstrate this, imagine some samples $\{\ve x^i\}_{i=1}^n$,
which at some time $t > 0$ look the same, 
i.e.~$\ve x_t = \ve x^i_t, \forall i$,
but have different optimal actions 
$\bar a^i_t \sim \Set T(\cdot|\ve x^i_t)$
according to tree-search, 
because tree-search can look into the future 
where the samples differ from each other.
However, there might be another action 
$\bar a^*_t \neq \bar a^i_t, \forall i$,
which is sub-optimal for each sample but optimal for their combination:
\begin{align}
	p_\theta\big(y^i| P(\ve x^i_t, \bar a^i_t)\big) 
	> p_\theta\big(y^i| P(\ve x^i_t, \bar a^*_t)\big) \,, 
	\quad \text{but} \quad
	\smallsum{j=1}{n}\, p_\theta\big(y^j | P(\ve x^j_t, \bar a^i_t)\big) < 
	\smallsum{j=1}{n}\, p_\theta\big(y^j| P(\ve x^j_t, \bar a^*_t)\big) \,, 
	\quad	\forall i\,.
\end{align}
For optimal classification of unknown samples $\ve x_t$, 
we should aim to predict $\bar a^*_t$, not $\bar a^i_t$.
This sounds like the ``joint tree search'' procedure that Julia
has implemented, but I would like to point out that this is
only optimal when $\ve x_t = \ve x^i_t, \forall i$,
that is, when there is no difference in the observations.
Instead, I would propose to average the tree search {\em only} over
the samples that have the same observation as current sample $\ve x_t$, 
i.e.~$\Set X(\ve x_t) := \big\{ i \,\big|\, 
\|\ve x_t^i - \ve x_t \| < \epsilon, 1 \leq i \leq n\big\}$.

It took me a while to come up with the value definition
of the corresponding tree, so please check the following carefully.
A node is characterized by the visible features $\ve x_t$
and has a list of children $\Set C_a$ for each explored action $a \in \Set A$. 
Those children $\ve x' \in \Set C_a$ 
are the result of revealing feature $a$ 
in a sample $\ve x^i_t$ from the set $i \in \Set X(\ve x_t)$
that is consistent with $\ve x_t$. 
As multiple paths can lead to the same node $\ve x'$,
children should be stored in a hash table to break transpositions.
With every forward path choosing $a$ in this node, 
a random (or enumerated, or heuristic) 
consistent sample $i \in \Set X(\ve x_t)$ is chosen
and the successor $\ve x' \sim P(\ve x^i_t, a)$ determined. 
If $\ve x'$ is not already in $\Set C_a$,
it is recalled from the hash table and added. 
This way, $\Set C_a$ will converge to the set of 
successors to all consistent samples.
Leafs simply have no children, 
i.e.~$\Set C_a = \varnothing, \forall a \in \Set A$.
After the forward pass has reached a leaf,
the backwards pass adjusts the value $V(\ve x_t)$
of all nodes along the sampled path: 
\begin{align}
	V(\ve x_t) \quad := \quad 
	\E\Big[\ln p_\theta(y^i |\, \ve x_{t}) 
		\,\Big|\, i \in \Set X(\ve x_t) \Big]
	+
	\max_{a \in \Set A} 
	\E\Big[ V(\ve x') \,\Big|\,	\ve x' \in \Set C_a	\Big]  \,.
\end{align}
Note that the average ``reward'' for classifying $\ve x_t$
does not depend on the choice of action $a$
and can be computed once at the creation of the node.
An obvious extension is to choose the executed action
in the forward pass with some heuristic,
e.g.~with the learned $\pi_\phi$. 

% ==============================================================================
\section{Complexity Analysis}


% ==============================================================================
\section{Network Architecture}
\begin{itemize}
\item
Feature encoding: Shared layers (a shared encoder) for the feature power set feed into both the prediction classifier and the feature acquisition policy.
\item
Initialization of the classifier: Pretrain the classifier $p_\theta$ with fully observed features and dropout regularization, in order to speed up training.
\end{itemize}


%\newpage
% ==============================================================================
%\def\FormatName#1{\IfSubStr{#1}{B\"ohmer}{\textbf{#1}}{#1}}
\def\FormatName#1{#1}
\bibliographystyle{highlight}
{\footnotesize\bibliography{references}}

\end{document}
